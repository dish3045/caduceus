{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dish3045/caduceus/blob/main/final_training_with_100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aUggkjhnWTv"
      },
      "outputs": [],
      "source": [
        "!pip install --force-reinstall transformers datasets evaluate scikit-learn accelerate --no-build-isolation\n",
        "!pip uninstall -y torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install causal-conv1d==1.4.0 && pip install mamba-ssm==2.2.2"
      ],
      "metadata": {
        "id": "861oqzot18fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
        "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "hBxAINamqhUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "itMwTVaLqsCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"clinvar_sequence_disease_clean.csv\")\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "print(\"Sample rows:\")\n",
        "\n",
        "print(\"Test dataset shape:\", df.shape)\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "XqVxXeEYqpli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "all_labels = [d for x in df['disease_labels'] for d in x.split(\",\")]\n",
        "label_counts = Counter(all_labels)\n",
        "\n",
        "top_labels = [d for d, _ in label_counts.most_common(100)]\n",
        "label2id = {d: i for i, d in enumerate(top_labels)}\n",
        "id2label = {i: d for d, i in label2id.items()}\n",
        "num_labels = len(top_labels)\n",
        "\n",
        "print(\"Number of diseases kept:\", num_labels)\n",
        "print(\"Most common ones:\", list(label2id.keys())[:10])"
      ],
      "metadata": {
        "id": "JYypLmy5rSCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_labels(label_str):\n",
        "    y = [0] * num_labels\n",
        "    for d in label_str.split(\",\"):\n",
        "        if d in label2id:\n",
        "            y[label2id[d]] = 1\n",
        "    return y\n",
        "\n",
        "df[\"label_vec\"] = df[\"disease_labels\"].apply(encode_labels)"
      ],
      "metadata": {
        "id": "M0pxoqcurado"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=None)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=None)\n",
        "\n",
        "print(f\"Train size: {len(train_df)}\")\n",
        "print(f\"Val size: {len(val_df)}\")\n",
        "print(f\"Test size: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "klv8e_z-rdix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model_name = \"kuleshov-group/caduceus-ps_seqlen-131k_d_model-256_n_layer-16\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "backbone = AutoModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "print(f\"Backbone hidden size: {backbone.config.d_model}\")\n",
        "print(f\"Backbone vocab size: {backbone.config.vocab_size}\")"
      ],
      "metadata": {
        "id": "41vuCN_1rmsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DNADataset(Dataset):\n",
        "    def __init__(self, sequences, labels, tokenizer, max_len=512):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.sequences[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if len(seq) > self.max_len:\n",
        "            seq = seq[:self.max_len]\n",
        "\n",
        "        try:\n",
        "            encoding = self.tokenizer(\n",
        "                seq,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_len,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            input_ids = encoding['input_ids'].squeeze()\n",
        "\n",
        "            if 'attention_mask' in encoding:\n",
        "                attention_mask = encoding['attention_mask'].squeeze()\n",
        "            else:\n",
        "                pad_token_id = getattr(self.tokenizer, 'pad_token_id', 0)\n",
        "                attention_mask = (input_ids != pad_token_id).long()\n",
        "\n",
        "        except Exception as e:\n",
        "            DNA_VOCAB = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3, \"N\": 4}\n",
        "\n",
        "            token_ids = [DNA_VOCAB.get(char.upper(), 4) for char in seq]\n",
        "\n",
        "            if len(token_ids) < self.max_len:\n",
        "                original_len = len(token_ids)\n",
        "                token_ids += [4] * (self.max_len - len(token_ids))\n",
        "                attention_mask = [1] * original_len + [0] * (self.max_len - original_len)\n",
        "            else:\n",
        "                token_ids = token_ids[:self.max_len]\n",
        "                attention_mask = [1] * self.max_len\n",
        "\n",
        "            input_ids = torch.tensor(token_ids, dtype=torch.long)\n",
        "            attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": torch.tensor(label, dtype=torch.float)\n",
        "        }"
      ],
      "metadata": {
        "id": "180O_GJ1tei-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 512\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "train_ds = DNADataset(train_df[\"sequence\"].tolist(), train_df[\"label_vec\"].tolist(), tokenizer, MAX_LEN)\n",
        "val_ds = DNADataset(val_df[\"sequence\"].tolist(), val_df[\"label_vec\"].tolist(), tokenizer, MAX_LEN)\n",
        "test_ds = DNADataset(test_df[\"sequence\"].tolist(), test_df[\"label_vec\"].tolist(), tokenizer, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "y-zkf-vKtlbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTesting backbone model compatibility...\")\n",
        "sample_batch = next(iter(train_loader))\n",
        "ids = sample_batch[\"input_ids\"][:2].to(device)\n",
        "mask = sample_batch[\"attention_mask\"][:2].to(device)\n",
        "\n",
        "backbone_call_method = None\n",
        "try:\n",
        "    print(\"Testing backbone(input_ids=ids, attention_mask=mask)...\")\n",
        "    outputs = backbone(input_ids=ids, attention_mask=mask)\n",
        "    backbone_call_method = \"keyword_args\"\n",
        "    print(\"Keyword arguments work!\")\n",
        "except Exception as e:\n",
        "    print(f\"Keyword args failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        print(\"Testing backbone(ids)...\")\n",
        "        outputs = backbone(ids)\n",
        "        backbone_call_method = \"input_ids_only\"\n",
        "        print(\"Input IDs only works!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Input IDs only failed: {e}\")\n",
        "\n",
        "        try:\n",
        "            print(\"Testing backbone(ids, mask)...\")\n",
        "            outputs = backbone(ids, mask)\n",
        "            backbone_call_method = \"positional_args\"\n",
        "            print(\"Positional arguments work!\")\n",
        "        except Exception as e:\n",
        "            print(f\"All methods failed: {e}\")\n",
        "            raise\n",
        "\n",
        "print(\"\\nDetecting actual backbone output dimensions...\")\n",
        "with torch.no_grad():\n",
        "    if backbone_call_method == \"keyword_args\":\n",
        "        outputs = backbone(input_ids=ids, attention_mask=mask)\n",
        "    elif backbone_call_method == \"input_ids_only\":\n",
        "        outputs = backbone(ids)\n",
        "    else:\n",
        "        outputs = backbone(ids, mask)\n",
        "\n",
        "    if hasattr(outputs, 'last_hidden_state'):\n",
        "        hidden = outputs.last_hidden_state\n",
        "    elif isinstance(outputs, tuple):\n",
        "        hidden = outputs[0]\n",
        "    else:\n",
        "        hidden = outputs\n",
        "\n",
        "    pooled = hidden.mean(dim=1)\n",
        "    actual_hidden_size = pooled.shape[-1]\n",
        "\n",
        "print(f\"Config says d_model: {backbone.config.d_model}\")\n",
        "print(f\"Actual output size: {actual_hidden_size}\")\n",
        "print(f\"Hidden state shape: {hidden.shape}\")\n",
        "print(f\"Pooled shape: {pooled.shape}\")"
      ],
      "metadata": {
        "id": "7_0MnFZ4u_cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiseaseClassifier(nn.Module):\n",
        "    def __init__(self, backbone, num_labels, actual_hidden_size, backbone_call_method, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.backbone_call_method = backbone_call_method\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        hidden_size = actual_hidden_size\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.LayerNorm(hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size // 2, hidden_size // 4),\n",
        "            nn.LayerNorm(hidden_size // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size // 4, num_labels)\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for module in self.classifier:\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.xavier_normal_(module.weight)\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        if self.backbone_call_method == \"keyword_args\":\n",
        "            outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        elif self.backbone_call_method == \"input_ids_only\":\n",
        "            outputs = self.backbone(input_ids)\n",
        "        else:\n",
        "            outputs = self.backbone(input_ids, attention_mask)\n",
        "\n",
        "        if hasattr(outputs, 'last_hidden_state'):\n",
        "            hidden = outputs.last_hidden_state\n",
        "        elif isinstance(outputs, tuple):\n",
        "            hidden = outputs[0]\n",
        "        else:\n",
        "            hidden = outputs\n",
        "\n",
        "        if attention_mask is not None and self.backbone_call_method != \"input_ids_only\":\n",
        "            mask_expanded = attention_mask.unsqueeze(-1).expand(hidden.size()).float()\n",
        "            sum_hidden = torch.sum(hidden * mask_expanded, dim=1)\n",
        "            sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
        "            pooled = sum_hidden / sum_mask\n",
        "        else:\n",
        "            pooled = hidden.mean(dim=1)\n",
        "\n",
        "        pooled = self.dropout(pooled)\n",
        "        logits = self.classifier(pooled)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "gn34P5yMtpCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "U9qtVhOskBlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "checkpoint_path = \"dna_disease_classifier_final.pth\"\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"Loading model from {checkpoint_path}...\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    model = DiseaseClassifier(\n",
        "        backbone=backbone,\n",
        "        num_labels=checkpoint['model_config']['num_labels'],\n",
        "        actual_hidden_size=checkpoint['model_config']['actual_hidden_size'],\n",
        "        backbone_call_method=checkpoint['model_config']['backbone_call_method'],\n",
        "        dropout_rate=checkpoint['model_config']['dropout_rate']\n",
        "    ).to(device)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(\"Model weights loaded successfully!\")\n",
        "\n",
        "else:\n",
        "    print(\"No checkpoint found, starting from scratch\")\n",
        "    model = DiseaseClassifier(backbone, num_labels, actual_hidden_size, backbone_call_method).to(device)\n",
        "\n",
        "try:\n",
        "    with torch.no_grad():\n",
        "        test_logits = model(ids, mask)\n",
        "        print(f\"SUCCESS! Output shape: {test_logits.shape}\")\n",
        "        print(f\"Expected shape: ({ids.shape[0]}, {num_labels})\")\n",
        "except Exception as e:\n",
        "    print(f\"Model forward pass failed: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "rkDba3rSx7er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    all_probs, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
        "            ids = batch[\"input_ids\"].to(device)\n",
        "            mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            logits = model(ids, mask)\n",
        "            probs = torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "            all_probs.append(probs)\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    all_probs = np.vstack(all_probs)\n",
        "    all_labels = np.vstack(all_labels)\n",
        "\n",
        "    best_f1 = 0\n",
        "    best_threshold = 0.5\n",
        "\n",
        "    for thresh in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
        "        bin_preds = (all_probs >= thresh).astype(int)\n",
        "        f1 = f1_score(all_labels, bin_preds, average=\"micro\", zero_division=0)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_threshold = thresh\n",
        "\n",
        "    bin_preds = (all_probs >= best_threshold).astype(int)\n",
        "\n",
        "    return {\n",
        "        \"threshold\": best_threshold,\n",
        "        \"f1_micro\": f1_score(all_labels, bin_preds, average=\"micro\", zero_division=0),\n",
        "        \"f1_macro\": f1_score(all_labels, bin_preds, average=\"macro\", zero_division=0),\n",
        "        \"precision\": precision_score(all_labels, bin_preds, average=\"macro\", zero_division=0),\n",
        "        \"recall\": recall_score(all_labels, bin_preds, average=\"macro\", zero_division=0),\n",
        "        \"accuracy\": (bin_preds == all_labels).mean()\n",
        "    }"
      ],
      "metadata": {
        "id": "bxH6pPE-ttEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels = []\n",
        "for batch in train_loader:\n",
        "    all_labels.append(batch[\"labels\"])\n",
        "all_labels = torch.cat(all_labels, dim=0).float()\n",
        "\n",
        "pos_counts = all_labels.sum(dim=0)\n",
        "neg_counts = all_labels.size(0) - pos_counts\n",
        "pos_weight = torch.clamp(neg_counts / (pos_counts + 1e-5), min=0.1, max=10.0).to(device)\n",
        "\n",
        "print(f\"Positive weights range: {pos_weight.min():.2f} - {pos_weight.max():.2f}\")\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)\n",
        "\n",
        "num_epochs = 2 #Running 2 epochs at a time to make sure colab doesn't crash in between and fuck us over :)\n",
        "total_steps = len(train_loader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * total_steps),\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining setup:\")\n",
        "print(f\"  Epochs: {num_epochs}\")\n",
        "print(f\"  Steps per epoch: {len(train_loader)}\")\n",
        "print(f\"  Total steps: {total_steps}\")\n",
        "print(f\"  Backbone call method: {backbone_call_method}\")\n",
        "print(f\"  Actual hidden size: {actual_hidden_size}\")\n",
        "\n",
        "best_val_f1 = 0\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        ids = batch[\"input_ids\"].to(device)\n",
        "        mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(ids, mask)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f'{total_loss/num_batches:.4f}',\n",
        "            'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
        "        })\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} - Average Loss: {total_loss/num_batches:.4f}\")\n",
        "\n",
        "    val_metrics = evaluate(model, val_loader, device)\n",
        "    print(f\"Validation: {val_metrics}\")\n",
        "\n",
        "    if val_metrics[\"f1_micro\"] > best_val_f1:\n",
        "        best_val_f1 = val_metrics[\"f1_micro\"]\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(f\"New best F1: {best_val_f1:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_metrics = evaluate(model, test_loader, device)\n",
        "print(f\"Test metrics: {test_metrics}\")\n",
        "print(f\"F1:     {test_metrics['f1_micro']:.1%}\")\n",
        "\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "IIvwkNYptur9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SAVING BEST MODEL FOR LOCAL USE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "final_model_save = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'model_config': {\n",
        "        'num_labels': num_labels,\n",
        "        'actual_hidden_size': actual_hidden_size,\n",
        "        'backbone_call_method': backbone_call_method,\n",
        "        'max_len': MAX_LEN,\n",
        "        'dropout_rate': 0.3\n",
        "    },\n",
        "    'label_mappings': {\n",
        "        'label2id': label2id,\n",
        "        'id2label': id2label,\n",
        "        'top_labels': top_labels\n",
        "    },\n",
        "    'backbone_model_name': model_name,\n",
        "    'final_metrics': {\n",
        "        'best_val_f1': best_val_f1,\n",
        "        'test_metrics': test_metrics\n",
        "    },\n",
        "    'training_info': {\n",
        "        'num_epochs': num_epochs,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'max_len': MAX_LEN\n",
        "    }\n",
        "}\n",
        "\n",
        "torch.save(final_model_save, 'dna_disease_classifier_final.pth')\n",
        "print(f\"Complete model saved as 'dna_disease_classifier_final.pth'\")\n",
        "print(f\"File size: {os.path.getsize('dna_disease_classifier_final.pth') / (1024*1024):.1f} MB\")\n",
        "\n",
        "# If running in Google Colab, download the file\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download('dna_disease_classifier_final.pth')\n",
        "    print(\"Model file downloaded to your computer!\")\n",
        "except ImportError:\n",
        "    print(\"Model saved locally in your current directory\")\n",
        "\n",
        "print(\"\\nModel Summary:\")\n",
        "print(f\"- Trained on {num_labels} disease types\")\n",
        "print(f\"- Best validation F1: {best_val_f1:.4f}\")\n",
        "print(f\"- Test F1: {test_metrics['f1_micro']:.4f}\")\n",
        "print(f\"- Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "metadata": {
        "id": "X0NqzGn2Z4il"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}